{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import struct\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "class TPMNDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    TMPN Click-Through Rate Prediction Dataset\n",
    "\n",
    "    :param dataset_path: tpmn train path\n",
    "    :param cache_path: lmdb cache path\n",
    "    :param rebuild_cache: If True, lmdb cache is refreshed\n",
    "    :param min_threshold: infrequent feature threshold\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path=None, cache_path='.tpmn_train', rebuild_cache=False, min_threshold=4):\n",
    "        self.new_dict = {}\n",
    "        self.char_index = 1\n",
    "        self.NUM_FEATS = 31 # sample 17, imbalance 23, june 31, june_sampling 19, june_sample(add for LSTM) 34\n",
    "        self.min_threshold = min_threshold\n",
    "        \n",
    "        if rebuild_cache or not Path(cache_path).exists():\n",
    "            shutil.rmtree(cache_path, ignore_errors=True)\n",
    "            if dataset_path is None:\n",
    "                raise ValueError('create cache: failed: dataset_path is None')\n",
    "            \n",
    "            self.__build_cache(dataset_path, cache_path)\n",
    "            # pickle.dump(self.__build_cache(dataset_path, cache_path), open(cache_path, 'wb'))\n",
    "        self.env = lmdb.open(cache_path, create=False, lock=False, readonly=True)\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            self.length = txn.stat()['entries'] - 1\n",
    "        \n",
    "        cache_file = 'tpmn.pkl'\n",
    "        if os.path.isfile(cache_file) :\n",
    "            self.field_dims = pickle.load(open(cache_file, 'rb'))\n",
    "        else :\n",
    "            with self.env.begin(write=False) as txn:\n",
    "                self.field_dims = np.frombuffer(txn.get(b'field_dims'), dtype=np.uint32)\n",
    "            pickle.dump(self.field_dims, open(cache_file, 'wb'))\n",
    "        ## loading new_dict     \n",
    "        with open('new_dict.pkl', 'rb') as f :\n",
    "            self.new_dict = pickle.load(f)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            np_array = np.frombuffer(\n",
    "                txn.get(struct.pack('>I', index)), dtype=np.uint32).astype(dtype=np.long)\n",
    "        \n",
    "        #return np_array[1:], np_array[0], np_array[self.NUM_FEATS+1:]\n",
    "        return np_array[1:self.NUM_FEATS+1], np_array[0], np_array[self.NUM_FEATS+1:] # feature, click, additional1, 2\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __build_cache(self, path, cache_path):\n",
    "        feat_mapper, defaults = self.__get_feat_mapper(path)\n",
    "        \n",
    "        with lmdb.open(cache_path, map_size=int(1e11)) as env:\n",
    "            field_dims = np.zeros(self.NUM_FEATS, dtype=np.uint32) \n",
    "            for i, fm in feat_mapper.items():\n",
    "                field_dims[i-1] = len(fm) + 1 # use length dict, determine field_dims\n",
    "            with env.begin(write=True) as txn:\n",
    "                txn.put(b'field_dims', field_dims.tobytes())\n",
    "            for buffer in self.__yield_buffer(path, feat_mapper, defaults):\n",
    "                with env.begin(write=True) as txn:\n",
    "                    for key, value in buffer:\n",
    "                        txn.put(key, value)\n",
    "            ## new_dict            \n",
    "            with open('new_dict.pkl', 'wb') as f :\n",
    "                pickle.dump(self.new_dict, f)\n",
    "                \n",
    "            \n",
    "        \n",
    "    def __get_feat_mapper(self, path):\n",
    "        feat_cnts = defaultdict(lambda: defaultdict(int))\n",
    "        with open(path) as f:\n",
    "            f.readline()\n",
    "            pbar = tqdm(f, mininterval=1, smoothing=0.1)\n",
    "            pbar.set_description('Create tpmn dataset cache: counting features')\n",
    "            for line in pbar:\n",
    "                values = line.rstrip('\\n').split(',')\n",
    "                if len(values) != self.NUM_FEATS + 1: \n",
    "                    continue\n",
    "                for i in range(1, self.NUM_FEATS + 1): \n",
    "                    feat_cnts[i][values[i]] += 1 \n",
    "                \n",
    "        # 여기서 random 발생하는듯..?\n",
    "        feat_mapper = {i: {feat for feat, c in cnt.items() if c >= self.min_threshold} for i, cnt in feat_cnts.items()} \n",
    "        feat_mapper = {i: {feat: idx for idx, feat in enumerate(cnt)} for i, cnt in feat_mapper.items()}\n",
    "        defaults = {i: len(cnt) for i, cnt in feat_mapper.items()}\n",
    "        \n",
    "        return feat_mapper, defaults # key feature num, value feature value + count\n",
    "\n",
    "    \n",
    "    def __yield_buffer(self, path, feat_mapper, defaults, buffer_size=int(1e5)):\n",
    "        item_idx = 0\n",
    "        buffer = list()\n",
    "        with open(path) as f:\n",
    "            f.readline()\n",
    "            pbar = tqdm(f, mininterval=1, smoothing=0.1)\n",
    "            pbar.set_description('Create tpmn dataset cache: setup lmdb')\n",
    "                \n",
    "            for line in pbar:\n",
    "                values = line.rstrip('\\n').split(',') \n",
    "                \n",
    "                if len(values) != self.NUM_FEATS + 1: \n",
    "                    continue\n",
    "                np_array = np.zeros(self.NUM_FEATS + 1, dtype=np.uint32) \n",
    "                np_array[0] = int(values[0]) # 0 : click\n",
    "                for i in range(1, self.NUM_FEATS + 1): \n",
    "                    np_array[i] = feat_mapper[i].get(values[i], defaults[i])\n",
    "                \n",
    "                ### Insert encoded ifa and ip numbers into lmdb\n",
    "                char_list = list(values[2] + values[5] + values[13])\n",
    "                \n",
    "                for char in char_list:\n",
    "                    if char not in self.new_dict :\n",
    "                        self.new_dict[char] = self.char_index\n",
    "                        self.char_index += 1\n",
    "                max_appbundle = 149\n",
    "                max_carrier = 50\n",
    "                max_make = 38\n",
    "                \n",
    "                encoded_appbundle = np.array([self.new_dict[char] for char in list(values[2])] + (max_appbundle - len(list(values[2]))) * [0], dtype=np.uint32)\n",
    "                encoded_carrier = np.array([self.new_dict[char] for char in list(values[5])] + (max_carrier - len(list(values[5]))) * [0], dtype=np.uint32)\n",
    "                encoded_make = np.array([self.new_dict[char] for char in list(values[13])] + (max_make - len(list(values[13]))) * [0], dtype=np.uint32)\n",
    "                np_array = np.concatenate([np_array, encoded_appbundle ,encoded_carrier, encoded_make]).astype(dtype=np.uint32)\n",
    "                \n",
    "                #if len(np_array) != 79: # click 1 + feature 23 + encoded ifa 40 + encoded ip 15 = 79\n",
    "                #    print(\"Array size error\")\n",
    "                    \n",
    "                buffer.append((struct.pack('>I', item_idx), np_array.tobytes())) # unsiged int\n",
    "                item_idx += 1\n",
    "                if item_idx % buffer_size == 0:\n",
    "                    yield buffer\n",
    "                    buffer.clear()\n",
    "            yield buffer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
